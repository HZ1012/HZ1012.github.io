---
layout:     post                    # 使用的布局（不需要改）
title:      集成学习             # 标题 
# subtitle:    #副标题
date:       2022-02-23              # 时间
author:     HZ                      # 作者
header-img: img/data science.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 数据科学
---

>本文转载[Jason Brownleed的英文博客](Machine Learning Mastery)

### 成学习]([Stacking Ensemble Machine Learning With Python (machinelearningmastery.com)](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/))

#### [生长和修剪集成]([Growing and Pruning Ensembles in Python (machinelearningmastery.com)](https://machinelearningmastery.com/growing-and-pruning-ensembles-in-python/))

尽管 ensemble 可能有大量的 ensemble 成员，但很难知道 ensemble 正在使用最好的成员组合。 例如，不是简单地使用所有成员，而是可以通过添加更多不同的模型类型或删除一个或多个模型来获得更好的结果。

这可以通过使用加权平均集成和使用优化算法为每个成员找到合适的权重来解决，允许某些成员具有零权重，从而有效地将他们从集成中移除。 加权平均集成的问题在于所有模型仍然是集成的一部分，可能需要比开发和维护更复杂的集成。

另一种方法是优化集成本身的组成。 自动选择或优化集成成员的一般方法称为集成选择。

两种常见的方法包括集成增长和集成修剪:

- **Ensemble Growing**: 将成员添加到集合中，直到观察不到进一步的改进。
- **Ensemble Pruning**: 从整体中移除成员，直到观察不到进一步的改进。

优点:

它可能导致一个具有更小规模（较低复杂性）的集成和/或一个具有更好预测性能的集成。 有时，如果可以在模型复杂性和由此产生的维护负担大幅下降的情况下实现性能的小幅下降是可取的。 或者，在某些项目中，预测技能比所有其他问题都更重要，并且集成选择提供了另一种策略来尝试从贡献模型中获得最大收益

使用说明：

在预期少量集成成员表现更好的情况下，出于计算效率原因，集成增长可能是首选，而在预期大量集成成员表现更好的情况下，集成修剪将更有效。

简单的贪婪集成增长和剪枝与逐步特征选择技术有很多共同点，例如在回归中使用的那些技术（例如所谓的逐步回归）。

可以使用更复杂的技术，例如根据他们在数据集上的独立表现来选择要添加到集合中或从集合中删除的成员，或者甚至通过使用全局搜索过程，试图找到能够获得最佳总体性能的集合成员组合。

#### [投票集成]([How to Develop Voting Ensembles With Python (machinelearningmastery.com)](https://machinelearningmastery.com/voting-ensembles-with-python/))

投票是一种集成机器学习算法。 对于回归，投票集成涉及进行预测，该预测是多个其他回归模型的平均值。 在分类中：

**硬投票**：直接用类别值，少数服从多数。
**软投票**：各自分类器的概率值进行加权平均

一个投票集合可以被认为是一个元模型，一个模型的模型。
作为元模型，它可以与任何现有训练机器学习模型的集合一起使用，并且现有模型不需要知道它们正在集成中使用。 这意味着您可以探索在任何拟合模型集或子集上使用投票集成来完成您的预测建模任务。 当您有两个或多个模型在预测建模任务上表现良好时，投票集成是合适的。 集成中使用的模型必须大多与他们的预测一致。

在以下情况下使用投票集成：

1. 集成中的所有模型通常具有相同的良好性能 
2. 整体中的所有模型大多已经同意。
3. 它比集成中使用的任何模型产生更好的性能。
4. 它导致比集成中使用的任何模型更低的方差。

当投票集成中使用的模型预测清晰的类标签时，硬投票是合适的。 当投票集成中使用的模型预测类成员的概率时，软投票是合适的。 软投票可用于本身不预测类成员概率的模型，尽管在用于集成之前可能需要校准它们的类概率分数（例如支持向量机、k-最近邻和决策树）。

投票集成不能保证提供比集成中使用的任何单个模型更好的性能。 如果集成中使用的任何给定模型的性能优于投票集成，则可能应该使用该模型而不是投票集成。 这并非总是如此。 投票集成可以在对单个模型的预测中提供较低的方差。 这可以从回归任务的预测误差的较低方差中看出。 这也可以从分类任务准确性的较低差异中看出。 **这种较低的方差可能会导致集成的平均性能较低，鉴于模型的稳定性或置信度较高，这可能是可取的。**

投票集成对于使用随机学习算法并在每次在同一数据集上训练时产生不同最终模型的机器学习模型特别有用。 一个例子是使用随机梯度下降拟合的神经网络。

投票集成的另一个特别有用的情况是将同一机器学习算法的多个拟合与略有不同的超参数组合在一起。
投票集成在以下情况下最有效： 

1. 结合使用随机学习算法训练的模型的多个拟合
2. 将模型的多个拟合与不同的超参数相结合。

投票集成的一个限制是它对所有模型都一视同仁，这意味着所有模型对预测的贡献相同。 如果某些模型在某些情况下很好，而在其他情况下很差，这就是一个问题。

投票集合的扩展：

1. 加权平均集成（混合）。
2. 堆叠泛化（stacking）。

#### [加权投票集成]([How to Develop a Weighted Average Ensemble With Python (machinelearningmastery.com)](https://machinelearningmastery.com/weighted-average-ensemble-with-python/))

加权平均或加权和集成是一种集成机器学习方法，它结合了来自多个模型的预测，其中每个模型的贡献与其能力或技能成比例地加权。

加权平均集成与投票集成相关。投票集成技术的一个限制是它假设集成中的所有模型都同样有效。 情况可能并非如此，因为某些模型可能比其他模型更好，尤其是当使用不同的机器学习算法来训练每个模型集成成员时。

 投票的另一种选择是假设集合成员的能力并不完全相同，相反，某些模型比其他模型更好，并且在进行预测时应该获得更多选票或更多席位。 这为加权总和或加权平均集成方法提供了动机。

加权平均预测涉及首先为每个集合成员分配一个固定的权重系数。 这可能是一个介于 0 和 1 之间的浮点值，表示权重的百分比。 它也可以是一个从 1 开始的整数，代表给每个模型的投票数。

我们可以看到，只要分数具有相同的尺度，并且权重具有相同的尺度并且正在最大化（意味着更大的权重更好），加权和会产生一个合理的值，反过来，加权平均值是也是明智的，这意味着结果的规模与分数的规模相匹配。

同样的方法可用于计算每个清晰类别标签的加权投票总和或每个类别标签在分类问题上的加权概率总和。

使用加权平均集成的挑战在于**如何为每个集成成员选择相对权重。**

有很多方法可以使用。 例如，可以根据每个模型的技能选择权重，例如分类准确率或负误差，其中权重大意味着模型性能更好。 可以在用于训练的数据集或保持数据集上计算性能，后者可能更相关。

每个模型的分数可以直接使用，也可以转换成不同的值，比如每个模型的相对排名。 另一种方法可能是使用搜索算法来测试不同的权重组合。


#### [超级学习器]([How to Develop Super Learner Ensembles in Python (machinelearningmastery.com)](https://machinelearningmastery.com/super-learner-ensemble-in-python/))

什么是超级学习器？

考虑到您已经在数据集上拟合了许多不同的算法，并且某些算法已经使用不同的配置进行了多次评估。 您的问题可能有数十或数百种不同的模型。 为什么不使用所有这些模型代替组中最好的模型？

这就是所谓的“超级学习器”集成算法背后的直觉。

超级学习器算法首先要预先定义数据的 k 折分割，然后在相同的数据分割上评估所有不同的算法和算法配置。 然后保留所有未折叠的预测并用于训练学习如何最好地组合预测的 。

超级学习器技术是称为“堆叠泛化”或简称“堆叠”的通用方法的一个示例，在应用机器学习中称为混合，因为通常使用线性模型作为元模型。

The procedure can be summarized as follows:

- Select a k-fold split of the training dataset.
- Select m base-models or model configurations.
- For each basemodel:
  - a. Evaluate using k-fold cross-validation.
  - b. Store all out-of-fold predictions.
  - c. Fit the model on the full training dataset and store.
- Fit a meta-model on the out-of-fold predictions.
- Evaluate the model on a holdout dataset or use model to make predictions.

![Diagram Showing the Data Flow of the Super Learner Algorithm](https://machinelearningmastery.com/wp-content/uploads/2019/10/Diagram-Showing-the-Data-Flow-of-the-Super-Learner-Algorithm.png)

元模型将基础模型的预测作为输入，并预测训练数据集的目标作为输出。

优点：可以集成多个模型结果，可用于回归和分类，模型性能优于任意一个单个模型

注：

1. 元模型通常很简单，提供了对基础模型所做预测的平滑解释。 因此，线性模型通常用作元模型，例如用于回归任务（预测数值）的线性回归和用于分类任务（预测类别标签）的逻辑回归。 虽然这很常见，但不是必需的。

2. 由于算法或评估过程的随机性，或数值精度的差异，您的结果可能会有所不同。考虑运行该示例几次，并比较平均结果。

3. 当多个不同的机器学习模型在一个数据集上有技能，但在不同的方式上有技能时，堆叠是合适的。 另一种说法是模型做出的预测或模型做出的预测误差不相关或相关性较低。

   基础模型通常是复杂多样的。 因此，使用一系列对如何解决预测建模任务做出截然不同假设的模型通常是一个好主意，例如线性模型、决策树、支持向量机、神经网络等。 其他集成算法也可以用作基础模型，例如随机森林。

4. 堆叠旨在提高建模性能，但不能保证在所有情况下都能带来改进。 实现性能改进取决于问题的复杂性，以及它是否被训练数据充分代表，是否足够复杂，以至于通过组合预测可以学习更多。 它还取决于基础模型的选择以及它们的预测（或错误）是否足够熟练且足够不相关。 如果基础模型的性能与堆叠集成一样好或更好，则应改用基础模型，因为它的复杂性较低（例如，它更易于描述、训练和维护）。

5. Q:为每个基础模型制作了最佳参数，并为每个基础模型制作了默认参数。 在我看来，如果每个基础模型都能达到更好的性能，那么元模型将通过训练数据与基础模型生成的预测相结合而获得更高的准确性。 但事实上，结果要差一些

   A:这是常见的。原因是高度调整的模型对小的变化很脆弱。

6. Q：如何从 Stacking Ensemble 中获取特征重要性？
   A: 顺便说一句，我不认为堆叠提供了这种能力。



#### [随机子空间集成]([How to Develop a Random Subspace Ensemble With Python (machinelearningmastery.com)](https://machinelearningmastery.com/random-subspace-ensemble-with-python/))

预测建模问题由一个或多个输入变量和一个目标变量组成。

 变量是数据中的一列，通常也称为特征。 我们可以将所有输入特征一起视为定义一个 n 维向量空间，其中 n 是输入特征的数量，每个示例（数据的输入行）是特征空间中的一个点。

这是机器学习中的一个常见概念，随着输入特征空间变大，空间中点之间的距离增加，通常称为维数灾难。

因此，输入特征的子集可以被认为是输入特征空间或子空间的子集。

选择特征是定义输入特征空间的子空间的一种方式。 例如，特征选择是指通过选择要保留的特征子集或要删除的特征子集来减少输入特征空间的维数的尝试，通常基于它们与目标变量的关系。

或者，我们可以选择输入特征的随机子集来定义随机子空间。 这可以用作集成学习算法的基础，其中模型可以适合每个随机特征子空间。 这被称为随机子空间系综或随机子空间方法。

因此，随机子空间集成与引导程序聚合（装袋）相关，它通过在训练数据集的不同随机样本上训练每个模型（通常是决策树）并替换（例如引导程序采样方法）来引入多样性。 **随机森林集成也可以被认为是装袋和随机子集集成方法的混合体**。

随机子空间方法可用于任何机器学习算法，尽管它非常适合对输入特征的大变化敏感的模型，例如决策树和 k 最近邻。

它适用于具有大量输入特征的数据集，因为它可以带来良好的性能和良好的效率。 **如果数据集包含许多不相关的输入特征，最好使用特征选择作为数据准备技术，因为子空间中不相关特征的普遍存在可能会损害集成的性能**。

随机子空间集成超参数:

1. 探索树的数量

2. 探索特征的数量

3. 探索替代算法

   决策树是随机子空间集成中最常用的算法。这样做的原因是它们易于配置并且可以很好地解决大多数问题。

   其他算法可用于构造随机子空间，并且必须配置为具有适度的高方差。 一个例子是 k-最近邻算法，其中 k 值可以设置为一个较低的值。

   集成中使用的算法通过“base_estimator”参数指定，并且必须设置为要使用的算法和算法配置的实例。

#### [动态分类器选择(DCS)]([Dynamic Classifier Selection Ensembles in Python (machinelearningmastery.com)](https://machinelearningmastery.com/dynamic-classifier-selection-in-python/))

动态分类器选择是一种用于分类预测建模的集成学习算法。

它是从许多训练有素的模型中选择一个以根据输入的特定细节进行预测的算法。

鉴于 DCS 中使用了多个模型，它被认为是一种集成学习技术。

动态分类器选择算法通常涉及以某种方式对输入特征空间进行分区，并分配特定模型来负责对每个分区进行预测。 有多种不同的 DCS 算法，研究工作主要集中在如何评估分类器并将其分配给输入空间的特定区域。

一种早期流行的方法涉及首先在训练数据集上拟合一组小的、多样化的分类模型。 当需要进行预测时，首先使用 k-最近邻 (kNN) 算法从训练数据集中找到与该示例匹配的 k 个最相似的示例。 然后在 k 个训练样本的邻居上评估模型中每个先前拟合的分类器，并选择表现最好的分类器来对新样本进行预测。

作者描述了两种选择单个分类器模型以对给定输入示例进行预测的方法，它们是：

- **Local Accuracy**, often referred to as LA or Overall Local Accuracy (OLA).
- **Class Accuracy**, often referred to as CA or Local Class Accuracy (LCA).

局部精度 (OLA) 涉及评估每个模型在 k 个训练示例的邻域上的分类精度。 然后选择在该邻域中表现最好的模型来对新示例进行预测。

分类准确度 (LCA) 涉及使用每个模型对新示例进行预测并记录预测的类。 然后，评估每个模型在 k 个训练样例的邻居上的准确性，并选择对其在新样例上预测的类别具有最佳技能的模型并返回其预测。

#### [动态集成选择(DES)]([Dynamic Ensemble Selection (DES) for Classification in Python (machinelearningmastery.com)](https://machinelearningmastery.com/dynamic-ensemble-selection-in-python/))

DCS 的一个自然扩展是动态选择一个或多个模型以进行预测的算法。 也就是说，动态选择分类器的子集或集合。 这些技术被称为动态集成选择或 DES。

动态集成选择算法的操作与 DCS 算法非常相似，不同之处在于使用来自多个分类器模型的投票而不是单个最佳模型进行预测。 实际上，输入特征空间的每个区域都由在该区域表现最佳的模型子集拥有。

KNORA 由 Albert Ko 等人描述。 在他们 2008 年题为“从动态分类器选择到动态集成选择”的论文中。 它是 DCS-LA 的扩展，它选择在邻域上表现良好的多个模型，然后使用多数投票将其预测组合起来，以做出最终的输出预测。

集成被认为是动态的，因为成员是根据需要预测的特定输入模式及时选择的。 这与静态相反，其中集成成员被选择一次，例如对模型中所有分类器的平均预测。

作者描述了 KNORA 的两个版本，包括 KNORA-Eliminate 和 KNORA-Union。

- **KNORA-Eliminate (KNORA-E)**: 在新示例的邻域上实现完美精度的分类器集合，并减小邻域大小，直到找到至少一个完美的分类器。
- **KNORA-Union (KNORA-U)**: 所有分类器的集合，通过加权投票对邻域进行至少一个正确的预测，并且投票与邻域的准确性成正比。

KNORA 的超参数调优：

1. 用于模型局部评估的 k 最近邻模型中的 k 值

   k 值控制邻域的大小，重要的是将其设置为适合数据集的值，特别是特征空间中的样本密度。 值太小意味着训练集中的相关示例可能会被排除在邻域之外，而值太大可能意味着信号被太多示例冲掉了。

2. 如何使用自定义分类器池

   除了指定分类器池之外，还可以从 scikit-learn 库中指定单个集成算法，KNORA 算法将自动使用内部集成成员作为分类器。
