---
layout:     post                    # 使用的布局（不需要改）
title:      异常点检测               # 标题 
# subtitle:    #副标题
date:       2022-02-23              # 时间
author:     HZ                      # 作者
header-img: img/data science.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 数据科学
---

>本文简述了异常点检测的使用场景和算法，具体代码可见刘建平（附链接）相关博客

### 异常点检测

#### 适用情形：

一是在做特征工程的时候需要对异常的数据做过滤，防止对归一化等处理的结果产生影响。二是对没有标记输出的特征数据做筛选，找出异常的数据。三是对有标记输出的特征数据做二分类时，由于某些类别的训练样本非常少，**类别严重不平衡**，此时也可以考虑用非监督的异常点检测算法来做

#### 类别：

常用的异常点检测算法一般分为三类

第一类是基于统计学的方法来处理异常数据，这种方法一般会构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为异常点。比如特征工程中的[RobustScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler)方法，在做数据特征值缩放的时候，它会利用数据特征的分位数分布，将数据根据分位数划分为多段，只取中间段来做缩放，比如只取25%分位数到75%分位数的数据做缩放。这样减小了异常数据的影响。

第二类是基于聚类的方法来做异常点检测。这个很好理解，由于大部分聚类算法是基于数据特征的分布来做的，通常如果我们聚类后发现某些聚类簇的数据样本量比其他簇少很多，而且这个簇里数据的特征均值分布之类的值和其他簇也差异很大，这些簇里的样本点大部分时候都是异常点。比如[BIRCH聚类算法原理](https://www.cnblogs.com/pinard/p/6179132.html)和[DBSCAN密度聚类算法](http://www.cnblogs.com/pinard/p/6208966.html)都可以在聚类的同时做异常点的检测。

第三类是基于专门的异常点检测算法来做。这些算法不像聚类算法，检测异常点只是一个赠品，它们的目的就是专门检测异常点的，这类算法的代表是LOF,One Class SVM和[Isolation Forest](#随机森林 (Random forest))

LOF: 局部异常因子是一种尝试利用最近邻的思想进行异常值检测的技术。每个示例都根据其本地邻域的大小分配了一个关于其孤立程度或成为异常值的可能性的评分。那些得分最高的例子更有可能是异常值

这对于低维（特征很少）的特征空间很有效，尽管随着特征数量的增加它可能变得不那么可靠，称为维数灾难

One Class SVM：对于SVDD来说，我们期望所有不是异常的样本都是正类别，同时它采用一个超球体而不是一个超平面来做划分，该算法在特征空间中获得数据周围的球形边界，期望最小化这个超球体的体积，从而最小化异常点数据的影响

<img src="https://latex.codecogs.com/svg.image?&space;&space;\underbrace{min}_{r,o}V(r)&space;&plus;&space;C\sum\limits_{i=1}^m\xi_i" title=" \underbrace{min}_{r,o}V(r) + C\sum\limits_{i=1}^m\xi_i" />

<img src="https://latex.codecogs.com/svg.image?||x_i-o||_2&space;\leq&space;r&space;&plus;&space;\xi_i,\;\;&space;i=1,2,...m" title="||x_i-o||_2 \leq r + \xi_i,\;\; i=1,2,...m" />

#### 小结：

IForest目前是异常点检测最常用的算法之一，它的优点非常突出，它具有线性时间复杂度。因为是随机森林的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。由于每棵树都是互相独立生成的，因此可以部署在大规模分布式系统上来加速运算。对于目前大数据分析的趋势来说，它的好用是有原因的。

但是IForest也有一些缺点，比如不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度和该维度的随机一个特征，建完树后仍然有大量的维度没有被使用，导致算法可靠性降低。此时推荐降维后使用，或者考虑使用One Class SVM。

另外iForest仅对即全局稀疏点敏感，不擅长处理局部的相对稀疏点 ，这样在某些局部的异常点较多的时候检测可能不是很准。

而One Class SVM对于中小型的数据分析，尤其是训练样本不是特别海量的时候用起来经常会比iForest顺手，因此比较适合做原型分析。
