---
layout:     post                    # 使用的布局（不需要改）
title:      数据降维               # 标题 
# subtitle:    #副标题
date:       2022-02-23              # 时间
author:     HZ                      # 作者
header-img: img/data science.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 数据科学
---

>对数据降维技术的介绍和优缺点分析

### 数据降维

过多输入变量的问题：

机器学习算法的性能会因输入变量过多而降低

我们可以将表示 n 维特征空间维度的数据列和数据行视为该空间中的点。 这是对数据集的有用几何解释。

在特征空间中拥有大量维度可能意味着该空间的体积非常大，反过来，我们在该空间中的点（数据行）通常代表一个小且不具代表性的样本。

这会极大地影响适合具有许多输入特征的数据的机器学习算法的性能，通常称为“维度灾难”。 因此，通常需要减少输入特征的数量。

降维：

高维可能意味着数百、数千甚至数百万个输入变量。

更少的输入维度通常意味着机器学习模型中相应的参数更少或更简单的结构，称为自由度。 具有太多自由度的模型可能会过度拟合训练数据集，因此可能无法在新数据上表现良好。

希望有简单的模型可以很好地泛化，反过来，输入数据的输入变量很少。 对于输入数量和模型的自由度通常密切相关的线性模型尤其如此。

降维是一种在建模之前对数据执行的数据准备技术。 它可以在数据清理和数据缩放之后以及训练预测模型之前执行。

#### 降维方法 

[特征选择：](#特征选择)

也许最常见的是所谓的特征选择技术，它使用评分或统计方法来选择保留哪些特征和删除哪些特征。

矩阵分解：

来自线性代数的技术可用于降维。

具体来说，矩阵分解方法可用于将数据集矩阵缩减为其组成部分。示例包括特征分解和奇异值分解。然后可以对这些部分进行排序，并且可以选择这些部分的一个子集，以最好地捕获可用于表示数据集的矩阵的显着结构。

对成分进行排序的最常用方法是主成分分析，简称 PCA。

流形学习：

来自高维统计的技术也可用于降维。

这些技术有时被称为“流形学习”，用于创建高维数据的低维投影，通常用于数据可视化。该投影旨在创建数据集的低维表示，同时最好地保留数据中的显着结构或关系。流形学习技术的例子包括：

- [Kohonen Self-Organizing Map (SOM)](https://en.wikipedia.org/wiki/Self-organizing_map).
- [Sammons Mapping](https://en.wikipedia.org/wiki/Sammon_mapping)
- Multidimensional Scaling (MDS)
- t-distributed Stochastic Neighbor Embedding (t-SNE).

投影中的特征通常与原始列几乎没有关系，例如 它们没有列名，这会让初学者感到困惑。

自编码器方法：

可以构建深度学习神经网络来执行降维。

一种流行的方法称为自动编码器。 这涉及构建一个自监督学习问题，其中模型必须正确再现输入。

使用网络模型试图将数据流压缩到维度比原始输入数据少得多的瓶颈层。 在瓶颈之前并包括瓶颈的模型部分称为编码器，读取瓶颈输出并重建输入的模型部分称为解码器。训练后，解码器被丢弃，瓶颈的输出直接用作输入的降维。 由该编码器转换的输入然后可以输入另一个模型，不一定是神经网络模型。

编码器的输出是一种投影，与其他投影方法一样，瓶颈输出与原始输入变量没有直接关系，这使得它们难以解释。

![img](https://pic1.zhimg.com/80/v2-1499966759ac2e9b7cf07e473bda7e14_720w.jpg)

- **缺失值比率（Missing Value Ratio）**：如果数据集的缺失值太多，我们可以用这种方法减少变量数。
- **低方差滤波（Low Variance Filter）**：这个方法可以从数据集中识别和删除常量变量，方差小的变量对目标变量影响不大，所以可以放心删去。
- **高相关滤波（High Correlation filter）**：具有高相关性的一对变量会增加数据集中的多重共线性，所以用这种方法删去其中一个是有必要的。
- **随机森林**：这是最常用的降维方法之一，它会明确算出数据集中每个特征的重要性。
- **前向特征选择**和**反向特征消除**：这两种方法耗时较久，计算成本也都很高，所以只适用于输入变量较少的数据集。
- **因子分析**：这种方法适合数据集中存在高度相关的变量集的情况。
- **PCA**：这是处理线性数据最广泛使用的技术之一。
- **ICA**：我们可以用ICA将数据转换为独立的分量，使用更少的分量来描述数据。
- **ISOMAP**：适合非线性数据处理。
- **t-SNE**：也适合非线性数据处理，相较上一种方法，这种方法的可视化更直接。
- **UMAP**：适用于高维数据，与t-SNE相比，这种方法速度更快

建议：

没有最好的降维技术，也没有技术到问题的映射。相反，最好的方法是使用系统控制的实验来发现哪些降维技术与您选择的模型搭配使用时，会在您的数据集上产生最佳性能。

通常，线性代数和流形学习方法假设所有输入特征具有相同的尺度或分布。 这表明，如果输入变量具有不同的尺度或单位，则在使用这些方法之前对数据进行归一化或标准化是一种很好的做法。
