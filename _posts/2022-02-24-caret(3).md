---
layout:     post                    # 使用的布局（不需要改）
title:      分类回归集成包caret（3）+tidymodels            # 标题 
subtitle:   caret包中文介绍指南 #副标题
date:       2022-02-24              # 时间
author:     HZ                      # 作者
header-img: img/bg-1st.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - R应用
---

### 使用单变量过滤器的特征选择

#### 单变量过滤器

特征选择的另一种方法是使用简单的单变量统计方法预先筛选预测变量，然后仅使用那些在后续模型步骤中通过某些标准的预测变量。 **与递归选择类似，后续模型的交叉验证将有偏差，因为剩余的预测变量已经在数据集上进行了评估。 通过重采样进行的正确性能估计应包括特征选择步骤**。

例如，有人建议对于分类模型，可以通过进行某种 k 样本测试（其中 k 是类别数）来过滤预测变量，以查看预测变量的平均值是否在类别之间不同。 有时会使用 Wilcoxon 检验、t 检验和方差分析模型。 然后将在类别之间具有统计显着差异的预测变量用于建模。

caret函数 sbf（用于过滤器选择）可用于交叉验证此类特征选择方案。 与 rfe 类似，函数可以传递到 sbf 中用于计算组件：单变量过滤、模型拟合、预测和性能摘要（详细信息如下）。

该函数应用于整个训练集以及数据集的不同重采样版本。 由此，可以计算出适当考虑到特征选择步骤的可概括的性能估计。 此外，可以通过重采样跟踪预测器过滤器的结果，以了解过滤中的不确定性。

#### 基本语法

与 rfe 函数类似，sbf 的语法是：

```R
sbf(predictors, outcome, sbfControl = sbfControl(), ...)
## or
sbf(formula, data, sbfControl = sbfControl(), ...)
```

在这种情况下，使用 sbfControl 函数指定详细信息。 在这里，参数函数决定了不同的组件应该做什么。 这个参数应该有名为 filter、fit、pred 和 summary 的元素。

##### score function

该函数将预测变量和结果分别作为 x 和 y 对象的输入。 默认情况下，x 中的每个预测变量都单独传递给评分函数。 在这种情况下，该函数应该返回一个分数。 或者，可以使用 sbfControl 的多变量参数将所有预测变量暴露给函数。 在这种情况下，输出应该是一个命名的分数向量，其中名称对应于 x 的列名称。

有两个内置函数，称为 anovaScores 和 gamScores。  anovaScores 将结果视为自变量，将预测变量视为结果。 这样，零假设是不同类别的平均预测值相等。 对于回归，gamScores 使用广义可加模型将预测变量中的平滑样条拟合到结果，并测试以查看两者之间是否存在任何函数关系。 在每个函数中，p 值用作分数。

##### filter function

此函数将来自 score 函数的分数作为输入（在名为 score 的参数中）。 该函数还将训练集数据作为输入（参数称为 x 和 y）。 输出应该是一个命名的逻辑向量，其中的名称对应于 x 的列名。 值为 TRUE 的列将在后续模型中使用。

##### fit function

该组件与上述 rfe 特定功能非常相似。 对于 sbf，没有第一个或最后一个参数。 该函数应具有参数 x、y 和 ...。 x 中的数据已使用上述过滤器函数进行过滤。 拟合函数的输出应该是拟合模型。

对于某些数据集，没有预测器会在过滤器中幸存下来。 在这些情况下，无法计算带有预测变量的模型，但在最终结果中不应忽略缺乏可行的预测变量。 为了解决这个问题，caret 包含一个名为 nullModel 的模型函数，它拟合一个独立于任何预测变量的简单模型。 对于结果为数字的问题，该函数使用训练集结果的简单平均值来预测每个样本。 对于分类，模型使用训练数据中最普遍的类别来预测所有样本。

此函数可用于拟合组件函数，以在未选择预测变量的情况下“错误陷阱”。 例如，某些模型有几个内置函数。 对象 rfSBF 是一组函数，可用于拟合带有过滤的随机森林模型。 这里的 fit 函数使用 nullModel 来检查没有预测变量的情况：

```R
rfSBF$fit
## function (x, y, ...) 
## {
##     if (ncol(x) > 0) {
##         loadNamespace("randomForest")
##         randomForest::randomForest(x, y, ...)
##     }
##     else nullModel(y = y)
## }
## <bytecode: 0x7fa6cc2db540>
## <environment: namespace:caret>
```

##### summary and pred function

summary函数用于计算保留样本的模型性能。  pred 函数用于使用当前预测器集预测新样本。 这两个函数的参数和输出与前面描述的部分中讨论的汇总和预测函数相同。

#### 例子

回到 (Friedman, 1991) 中的示例，我们可以将另一个随机森林模型与使用前面描述的广义加性模型方法预先过滤的预测变量进行拟合。

```R
filterCtrl <- sbfControl(functions = rfSBF, method = "repeatedcv", repeats = 5)
set.seed(10)
rfWithFilter <- sbf(x, y, sbfControl = filterCtrl)
rfWithFilter

## 
## Selection By Filter
## 
## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) 
## 
## Resampling performance:
## 
##   RMSE Rsquared  MAE RMSESD RsquaredSD  MAESD
##  3.407   0.5589 2.86 0.5309     0.1782 0.5361
## 
## Using the training set, 6 variables were selected:
##    real2, real4, real5, bogus2, bogus17...
## 
## During resampling, the top 5 selected variables (out of a possible 13):
##    real2 (100%), real4 (100%), real5 (100%), bogus44 (76%), bogus2 (44%)
## 
## On average, 5.5 variables were selected (min = 4, max = 8)
```

在这种情况下，训练集表明应该在随机森林模型中使用 6，但重采样结果表明这个数字存在一些变化。 使用了一些信息性预测变量，但保留了其他一些错误的预测变量。

与 rfe 类似，也有predictors`, `densityplot`, `histogram` and `varImp的方法。

### 递归特征消除

#### 向后选择

首先，该算法使模型适合所有预测变量。 每个预测器都使用它对模型的重要性进行排名。 令 S 是一系列有序数字，这些数字是要保留的预测变量数量的候选值（S1 > S2，...）。 在特征选择的每次迭代中，Si 排名靠前的预测器被保留，模型被重新拟合并评估性能。 确定具有最佳性能的 Si 值，并使用顶部 Si 预测变量来拟合最终模型。 算法 1 有更完整的定义。

该算法有一个可选步骤（第 1.9 行），在该步骤中，预测器排名在缩减特征集的模型上重新计算。  Svetnik 等人 (2004) 表明，对于随机森林模型，在每一步重新计算排名时，性能都会下降。 但是，在初始排名不好的其他情况下（例如具有高度共线预测变量的线性模型），重新计算可以稍微提高性能。

![img](https://topepo.github.io/caret/premade/Algo1.png)

一个潜在的问题是对预测变量过度拟合，使得包装程序可以关注未来样本中未发现的训练数据的细微差别（即对预测器和样本过度拟合）。

例如，假设收集了大量无信息预测变量，并且其中一个这样的预测变量与结果随机相关。  RFE 算法会给这个变量一个很好的排名，并且预测误差（在同一数据集上）会降低。 需要进行不同的测试/验证才能发现该预测器没有提供信息。  Ambroise 和 McLachlan (2002) 将其称为“选择偏差”。

在当前的 RFE 算法中，训练数据至少用于三个目的：预测变量选择、模型拟合和性能评估。 除非样本数量很大，特别是与变量数量有关，否则一个静态训练集可能无法满足这些需求。

#### 重采样和外部验证

由于特征选择是模型构建过程的一部分，重采样方法（例如交叉验证、引导程序）应该在计算性能时考虑特征选择引起的可变性。 例如，算法 1 中的 RFE 过程可以在选择过程中估计第 1.7 行的模型性能。  Ambroise 和 McLachlan (2002) 以及 Svetnik 等人 (2004) 表明，不正确地使用重采样来衡量性能将导致模型在新样本上表现不佳。

为了获得包含由于特征选择引起的变化的性能估计，建议将算法 1 中的步骤封装在重采样的外层内（例如 10 倍交叉验证）。 算法 2 显示了使用重采样的算法版本。

 虽然这将提供更好的性能估计，但计算量更大。 对于可以访问具有多个处理器的机器的用户，算法 2（第 2.1 行）中的第一个 For 循环可以轻松并行化。 使用重采样的另一个复杂因素是每次迭代都会生成多个“最佳”预测变量列表。 乍一看，这似乎是一个缺点，但与基于单个固定数据集的排名相比，它确实提供了对预测变量重要性的更多概率评估。 在算法结束时，可以使用共识排名来确定要保留的最佳预测因子。

![img](https://topepo.github.io/caret/premade/Algo2.png)

#### 通过caret的递归特征消除

在caret中，算法 1 由函数 rfeIter 实现。 基于重采样的算法 2 在 rfe 函数中。 鉴于潜在的选择偏差问题，本文档重点介绍 rfe。 有几个参数：

* x，预测变量的矩阵或数据框，
* y,结果大小的向量（数字或因子），
* 大小，应测试的特定子集大小的整数向量（不需要包含 ncol(x)）
* rfeControl，可用于指定模型和预测、排名等方法的选项列表。

对于特定模型，必须在 rfeControl$functions 中指定一组函数。 下面的部分描述了这些子功能。 多个模型有许多预定义的函数集，包括：线性回归（在对象 lmFuncs 中）、随机森林 (rfFuncs)、朴素贝叶斯 (nbFuncs)、袋装树 (treebagFuncs) 和可用于 caret 的训练函数 (caretFuncs)。 如果模型具有必须在每次迭代时确定的调整参数，则后者很有用。

#### 一个例子

```R
library(caret)
library(mlbench)
library(Hmisc)
library(randomForest)
```

为了测试算法，使用了“Friedman 1”基准（Friedman，1991）。 方程产生了五个信息变量

![img](https://topepo.github.io/caret/premade/FEq.png)

在此处使用的模拟中：

```R
n <- 100
p <- 40
sigma <- 1
set.seed(1)
sim <- mlbench.friedman1(n, sd = sigma)
colnames(sim$x) <- c(paste("real", 1:5, sep = ""),
                     paste("bogus", 1:5, sep = ""))
bogus <- matrix(rnorm(n * p), nrow = n)
colnames(bogus) <- paste("bogus", 5+(1:ncol(bogus)), sep = "")
x <- cbind(sim$x, bogus)
y <- sim$y
```

在 50 个预测变量中，有 45 个纯噪声变量：5 个在 0 和1上是一致的 40 个是随机单变量标准正态。 预测变量居中并按比例缩放：

```R
normalization <- preProcess(x)
x <- predict(normalization, x)
x <- as.data.frame(x)
subsets <- c(1:5, 10, 15, 20, 25)
```

模拟将拟合子集大小为 25、20、15、10、5、4、3、2、1 的模型。

如前所述，为了拟合线性模型，可以使用 lmFuncs 函数集。 为此，使用 rfeControl 函数创建一个控制对象。 我们还指定应在算法 2 的第 2.1 行中使用重复的 10 折交叉验证。可以通过 rfeControl 的 number 参数（默认为 10）更改折叠数。 详细选项可防止产生大量输出。

```R
set.seed(10)

ctrl <- rfeControl(functions = lmFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)

lmProfile <- rfe(x, y,
                 sizes = subsets,
                 rfeControl = ctrl)

lmProfile

## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) 
## 
## Resampling performance over subset size:
## 
##  Variables  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD Selected
##          1 3.950   0.3790 3.381 0.6379     0.2149 0.5867         
##          2 3.552   0.4985 3.000 0.5820     0.2007 0.5807         
##          3 3.069   0.6107 2.593 0.6022     0.1582 0.5588         
##          4 2.889   0.6658 2.319 0.8208     0.1969 0.5852        *
##          5 2.949   0.6566 2.349 0.8012     0.1856 0.5599         
##         10 3.252   0.5965 2.628 0.8256     0.1781 0.6016         
##         15 3.405   0.5712 2.709 0.8862     0.1985 0.6603         
##         20 3.514   0.5562 2.799 0.9162     0.2048 0.7334         
##         25 3.700   0.5313 2.987 0.9095     0.1972 0.7500         
##         50 4.067   0.4756 3.268 0.8819     0.1908 0.7315         
## 
## The top 4 variables (out of 4):
##    real4, real5, real2, real1
```

输出显示最佳子集大小估计为 4 个预测变量。 该集合包括信息变量，但并未包括所有变量。  predictors 函数可用于获取在最终模型中选取的变量名称的文本字符串。  lmProfile 是一个“rfe”类的列表，它包含一个对象fit，它是具有剩余项的最终线性模型。 该模型可用于获得对未来或测试样本的预测。

```R
predictors(lmProfile)
## [1] "real4" "real5" "real2" "real1"
lmProfile$fit
## 
## Call:
## lm(formula = y ~ ., data = tmp)
## 
## Coefficients:
## (Intercept)        real4        real5        real2        real1  
##      14.613        2.857        1.965        1.625        1.359
head(lmProfile$resample)
##    Variables     RMSE  Rsquared      MAE    Resample
## 4          4 1.923763 0.9142474 1.640438 Fold01.Rep1
## 14         4 2.212266 0.8403133 1.845878 Fold02.Rep1
## 24         4 4.074172 0.5052766 3.095980 Fold03.Rep1
## 34         4 3.938895 0.3250410 2.992700 Fold04.Rep1
## 44         4 3.311426 0.6652186 2.195083 Fold05.Rep1
## 54         4 2.286320 0.6974626 1.840118 Fold06.Rep1
```

还有几种绘图方法可以将结果可视化。  plot(lmProfile) 生成跨不同子集大小的性能配置文件，如下图所示。

```R
trellis.par.set(caretTheme())
plot(lmProfile, type = c("g", "o"))
```

![img](https://topepo.github.io/caret/rfe/rfe_lmprofile-1.svg)

此外，重采样结果存储在子对象 lmProfile$resample 中，可以与多个lattice函数一起使用。 单变量点阵函数（densityplot`, `histogram）可用于绘制重采样分布，而双变量函数（xyplot、stripplot）可用于绘制不同子集大小的分布。 在后一种情况下，可以使用 rfeControl 中的选项 returnResamp`` = "all" 来保存所有重采样结果。 下面显示了随机森林模型的示例图像。

#### 辅助函数

要对任意模型使用特征消除，必须为算法 2 中的每个步骤将一组函数传递给 rfe。

本节定义这些函数并使用现有的随机森林函数作为说明性示例。  caret 包含一个名为 rfFuncs 的列表，但本文档将使用更简单的版本，以便更好地说明这些想法。 这里使用了一组简化的函数，称为 rfRFE。

##### summary

summary函数采用观察值和预测值并计算一个或多个性能指标（参见第 2.14 行）。 输入是一个包含 obs 和 pred 列的数据框。 输出应该是数字变量的命名向量。 请注意， rfe 函数的 metric 参数应引用 summary 输出的名称之一。 

#### fit

此函数基于当前数据集（第 2.3、2.9 和 2.17 行）构建模型。 函数的参数必须是：

x：具有适当变量子集的当前预测变量数据训练集 

y：当前结果数据（数字或因子向量）

 first：当前预测变量集是否具有所有可能变量的单个逻辑值（例如第 2.3 行） 

 last：类似于 first，但当最后一个模型与最终子集大小和预测变量相匹配时为 TRUE。  （第 2.17 行）...：在调用 rfe 时传递给 fit 函数的可选参数

该函数应返回一个可用于生成预测的模型对象。 对于随机森林，拟合函数很简单：

对于在每次迭代时不重新排序的特征选择，当所有预测变量都在模型中时，只需在第一次迭代时计算随机森林变量的重要性。 这可以使用importance``= first来完成。

#### pred function

此函数从当前模型（第 2.4 和 2.10 行）返回一个预测向量（数字或因子）。 输入参数必须是对象：拟合函数生成的模型 x：为保留样本设置的当前预测变量集

对于随机森林，该函数是 predict 函数的一个简单包装器：对于分类，确保预测的结果因子变量与输入数据具有相同的水平可能是一个好主意。

#### rank function

此函数用于按最重要到最不重要的顺序返回预测变量（第 2.5 行和第 2.11 行）。 输入是： 

object：拟合函数生成的模型

 x：训练样本的当前预测变量集 

y：当前训练结果 

该函数应该返回一个数据框，其中包含一个名为 var 的列，该列具有当前变量名称。 第一行应该是最重要的预测器等。其他列可以包含在输出中，并将在最终的 rfe 对象中返回。

对于随机森林，下面的函数使用 caret 的 varImp 函数来提取随机森林的重要性并对它们进行排序。 对于分类，randomForest 将为每个类生成一列重要性。 在这种情况下，默认排名函数按类别的平均重要性对预测变量进行排序。

#### selectSize function

此函数根据重采样输出（第 2.15 行）确定最佳预测变量数。 该函数的输入是： 

x：一个矩阵，其中包含性能指标和变量数量的列，称为Variables 

metric：要优化的性能指标的字符串（例如 RMSE、准确度） 

maximize：衡量指标是否存在的单个逻辑 应该最大化 这个函数应该返回一个对应于最优子集大小的整数。

caret为此提供了两个示例函数：pickSizeBest 和 pickSizeTolerance。 前者只是选择具有最佳值的子集大小。 后者考虑了整个配置文件，并尝试在不牺牲太多性能的情况下选择较小的子集大小。 例如，假设我们已经计算了一系列变量大小的 RMSE：

```R
example <- data.frame(RMSE = c(3.215, 2.819, 2.414, 2.144, 
                               2.014, 1.997, 2.025, 1.987, 
                               1.971, 2.055, 1.935, 1.999, 
                               2.047, 2.002, 1.895, 2.018),
                               Variables = 1:16)
```

这些如下图所示。 实心圆圈标识具有绝对最小 RMSE 的子集大小。 但是，有许多较小的子集产生大致相同的性能，但预测变量较少。 在这种情况下，对于较少的预测变量，我们可能能够接受稍大的误差。

pickSizeTolerance 确定绝对最佳值，然后确定其他点与该值的百分比差异。 在 RMSE 的情况下，这将是

![img](https://topepo.github.io/caret/premade/tol.png) 

 *RMSE{opt}*是绝对最佳错误率。 这些“公差”值绘制在底部面板中。 实心三角形是最佳值的 10% 以内的最小子集大小。

这种方法可以为许多基于树的模型产生良好的结果，例如随机森林，其中对于较大的子集大小存在良好性能的平台。 对于树，这通常是因为不重要的变量很少用于拆分并且不会显着影响性能。

```R
## 找到具有绝对最小 RMSE 的行
smallest <- pickSizeBest(example, metric = "RMSE", maximize = FALSE)
smallest
## [1] 5
## 现在是最小的 10% 以内的
within10Pct <- pickSizeTolerance(example, metric = "RMSE", tol = 10, maximize = FALSE)
within10Pct
## [1] 5

minRMSE <- min(example$RMSE)
example$Tolerance <- (example$RMSE - minRMSE)/minRMSE * 100   

## Plot the profile and the subsets selected using the 
## two different criteria

par(mfrow = c(2, 1), mar = c(3, 4, 1, 2))

plot(example$Variables[-c(smallest, within10Pct)], 
     example$RMSE[-c(smallest, within10Pct)],
     ylim = extendrange(example$RMSE),
     ylab = "RMSE", xlab = "Variables")

points(example$Variables[smallest], 
       example$RMSE[smallest], pch = 16, cex= 1.3)

points(example$Variables[within10Pct], 
       example$RMSE[within10Pct], pch = 17, cex= 1.3)
 
with(example, plot(Variables, Tolerance))
abline(h = 10, lty = 2, col = "darkgrey")
```

![img](https://topepo.github.io/caret/rfe/rfe_lmdens-1.svg)

#### selectVar  function

确定最佳子集大小后，此函数将用于计算所有重采样迭代中每个变量的最佳排名（第 2.16 行）。 该函数的输入是：

 y：每个重采样迭代和每个子集大小（由用户定义的秩函数生成）的变量重要性列表。 在该示例中，对于 10 个子集大小（包括原始子集）中的每一个，每个交叉验证组都保存了秩函数的输出。 如果没有在每次迭代时重新计算排名，则每次交叉验证迭代中的值将相同。

size：selectSize 函数返回的整数 该函数应该按照最重要到最不重要的顺序返回预测变量名称的字符串（长度大小）对于随机森林，仅使用第一个重要性计算（第 2.5 行），因为这些 是对全套预测变量的排名。 这些重要性被平均并返回最高的预测值。

请注意，如果在每次迭代时重新计算预测器排名（第 2.11 行），用户将需要编写自己的选择函数以使用其他排名。

#### 例子

对于随机森林，我们拟合了与线性模型相同的一系列模型大小。 此模型更改了跨子集大小保存所有重采样结果的选项，用于显示下图中的点阵图函数功能。

```R
ctrl$functions <- rfRFE
ctrl$returnResamp <- "all"
set.seed(10)
rfProfile <- rfe(x, y, sizes = subsets, rfeControl = ctrl)
rfProfile

## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) 
## 
## Resampling performance over subset size:
## 
##  Variables  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD Selected
##          1 4.667   0.2159 3.907 0.8779    0.20591 0.7889         
##          2 3.801   0.4082 3.225 0.5841    0.21832 0.5858         
##          3 3.157   0.6005 2.650 0.5302    0.14847 0.5156         
##          4 2.696   0.7646 2.277 0.4044    0.08625 0.3962        *
##          5 2.859   0.7553 2.385 0.4577    0.10529 0.4382         
##         10 3.061   0.7184 2.570 0.4378    0.13898 0.4106         
##         15 3.170   0.7035 2.671 0.4423    0.15140 0.4110         
##         20 3.327   0.6826 2.812 0.4469    0.16074 0.4117         
##         25 3.356   0.6729 2.843 0.4634    0.16947 0.4324         
##         50 3.525   0.6437 3.011 0.4597    0.17207 0.4196         
## 
## The top 4 variables (out of 4):
##    real4, real5, real2, real1
```

可以将重采样配置文件与单个重采样结果的图一起可视化：

```R
trellis.par.set(caretTheme())
plot1 <- plot(rfProfile, type = c("g", "o"))
plot2 <- plot(rfProfile, type = c("g", "o"), metric = "Rsquared")
print(plot1, split=c(1,1,1,2), more=TRUE)
print(plot2, split=c(1,2,1,2))
```

```R
plot1 <- xyplot(rfProfile, 
                type = c("g", "p", "smooth"), 
                ylab = "RMSE CV Estimates")
plot2 <- densityplot(rfProfile, 
                     subset = Variables < 5, 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "RMSE CV Estimates", 
                     pch = "|")
print(plot1, split=c(1,1,1,2), more=TRUE)
print(plot2, split=c(1,2,1,2))
```

![img](https://topepo.github.io/caret/rfe/rfe_rf_plot2-1.svg)

#### 使用recipe

recipe可用于指定模型项和可能需要的任何预处理。 而不是使用

```R
rfe(x = predictors, y = outcome)
```

现有recipe可以与包含预测变量和结果的数据框一起使用：

```R
rfe(recipe, data)
```

recipe在每个重采样中以与 train 执行 preProc 选项相同的方式准备。 但是，由于recipe可以执行各种不同的操作，因此存在一些潜在的复杂因素。 主要的缺陷是recipe可能涉及预测变量的创建和删除。 有许多步骤可以减少预测变量的数量，例如将因子合并到“其他”类别中的步骤、PCA 信号提取，以及用于接近零方差预测变量和高度相关预测变量的过滤器。 因此，可能很难知道有多少预测变量可用于完整模型。 此外，这个数字可能会在重采样的迭代之间有所不同。

为了说明，让我们使用血脑屏障数据，其中预测变量之间存在高度相关性。 一个简单的recipe可能是

```R
library(recipes)
library(tidyverse)

data(BloodBrain)

# combine into a single data frame
bbb <- bbbDescr
bbb$y <- logBBB

bbb_rec <- recipe(y ~ ., data = bbb) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_pca(all_predictors(), threshold = .95) 
```

最初，有 134 个预测变量，对于整个数据集，处理后的版本有：

```
prep(bbb_rec, training = bbb, retain = TRUE) %>% 
  juice(all_predictors()) %>% 
  ncol()
  
  ## [1] 28
```

调用 rfe 时，让我们从 28 开始最大子集大小：

```R
bbb_ctrl <- rfeControl(
  method = "repeatedcv",
  repeats = 5,
  functions = lmFuncs, 
  returnResamp = "all"
)

set.seed(36)
lm_rfe <- rfe(
  bbb_rec,
  data = bbb,
  sizes = 2:28,
  rfeControl = bbb_ctrl
)

ggplot(lm_rfe) + theme_bw()
```

![img](https://topepo.github.io/caret/rfe/rfe-rec-1.svg)

最大术语数的分布是什么：

```R
term_dist <- 
  lm_rfe$resample %>% 
  group_by(Resample) %>% 
  dplyr::summarize(max_terms = max(Variables))
table(term_dist$max_terms)

## 
## 27 28 29 
##  7 40  3
```

### 使用遗传算法进行特征选择

#### 遗传算法

遗传算法 (GA) 模仿达尔文自然选择的力量来寻找某些函数的最佳值 (Mitchell, 1998)。 创建一组初始候选解决方案并计算其相应的适应度值（其中值越大越好）。 这组解决方案称为总体，每个解决方案称为个体。 具有最佳适应度值的个体被随机组合以产生构成下一个种群的后代。 为此，个体被选择并进行交叉（模仿基因繁殖），并且还受到随机突变的影响。 这个过程一次又一次地重复，并且产生了许多代（即搜索过程的迭代），这些代应该创造出越来越好的解决方案。

对于特征选择，个体是编码为二进制的预测变量的子集； 一个特征要么包含在子集中，要么不包含在子集中。 适应度值是模型性能的某种度量，例如 RMSE 或分类准确度。 使用 GA 进行特征选择的一个问题是优化过程可能非常激进，并且它们有可能使 GA 过拟合预测器（很像之前针对 RFE 的讨论）。

#### 内部和外部效率评估

caret中的遗传算法代码在重采样迭代中重复进行特征空间的搜索。 首先，训练数据被拆分为控制函数中指定的任何重采样方法。 例如，如果选择 10 折交叉验证，则整个遗传算法将进行 10 次单独的验证。 对于第一折，十分之九的数据用于搜索，而剩余的十分之一用于估计外部性能，因为这些数据点未用于搜索。

 在遗传算法过程中，需要一个适应度的度量来指导搜索。 这是绩效的内部衡量标准。 在搜索过程中，可用的数据是顶级重采样选择的实例（例如上面提到的十分之九）。 一种常见的方法是进行另一个重采样程序。 另一种选择是使用一组保持样本来确定性能的内部估计（请参阅控制函数的保持参数）。 虽然速度更快，但更容易导致特征过度拟合，因此只能在有大量训练数据可用时使用。 另一个想法是使用惩罚指标（例如 AIC 统计量），但对于某些指标（例如 ROC 曲线下的面积），这可能不存在。

性能的内部估计最终会使子集过度拟合数据。 但是，由于搜索不使用外部估计，因此可以更好地评估过度拟合。 重采样后，此函数确定 GA 的最佳代数。

最后，在最后一次执行遗传算法搜索时使用整个数据集，最终模型建立在预测子集上，该预测子集与通过重采样确定的最佳代数相关（尽管更新函数可用于手动设置代数）。

#### 基本语法

该函数最基本的用法是：

```R
obj <- gafs(x = predictors, 
            y = outcome,
            iters = 100)
```

其中 

x：预测值的数据框或矩阵

 y：结果的因子或数值向量 

iters：GA 的代数 

这不是很具体。 所有的动作都在控制功能中。 这可用于指定要拟合的模型、如何进行预测和总结以及遗传操作。

假设我们想要拟合一个线性回归模型。 为此，我们可以使用 train 作为接口并通过 gafs 将参数传递给该函数：

```R
ctrl <- gafsControl(functions = caretGA)
obj <- gafs(x = predictors, 
            y = outcome,
            iters = 100,
            gafsControl = ctrl,
            ## 现在将选项传递给`train`
            method = "lm")
```

也可以传入其他选项，例如 preProcess。

gafsControl 的一些重要选项是：method、number、repeats、index、indexOut 等：类似于 train 顶部控制重采样

metric：这类似于 train 的选项，但在这种情况下，该值应该是一个命名向量，其中包含内部和外部指标的值。 如果未指定，则使用汇总函数返回的第一个值（请参阅下面的详细信息）并发出警告。 选项最大化也需要一个类似的二元素向量。 有关说明，请参阅此处的最后一个示例。

holdout：这是一个介于 [0, 1) 之间的数字，可用于保留样本以计算内部适应度值。 请注意，这与外部重采样步骤无关。 假设正在使用 10 倍的 CV。 在重采样迭代中，holdout 可用于对 90% 重采样数据的额外比例进行采样，以用于估计适应度。 这可能不是一个好主意，除非你有一个非常大的训练集并且想要避免内部重采样过程来估计适应度。

 allowParallel 和 genParallel：这些是控制应该在何处使用并行处理（如果有的话）的逻辑。 前者将并行化外部重采样，而后者并行化一代内的适应度计算。  allowParallel 几乎总是更有利。

有几组内置函数可用于 gafs：caretGA、rfGA 和 treebagGA。 第一个是简单的训练界面。 使用它时，如上所示，可以使用 ... 结构将参数传递给训练，并且性能的重采样估计可以用作内部适应度值。  rfGA 和 treebagGA 提供的函数避免使用 train ，它们的内部适应度估计来自使用模型生成的袋外估计。

caret中的 GA 实现使用来自 GA 包的底层代码（Scrucca，2013）。

#### 遗传算法示例

使用上一页的示例，其中有 5 个真实预测变量和 40 个噪声预测变量：

```R
library(mlbench)
n <- 100
p <- 40
sigma <- 1
set.seed(1)
sim <- mlbench.friedman1(n, sd = sigma)
colnames(sim$x) <- c(paste("real", 1:5, sep = ""),
                     paste("bogus", 1:5, sep = ""))
bogus <- matrix(rnorm(n * p), nrow = n)
colnames(bogus) <- paste("bogus", 5+(1:ncol(bogus)), sep = "")
x <- cbind(sim$x, bogus)
y <- sim$y
normalization <- preProcess(x)
x <- predict(normalization, x)
x <- as.data.frame(x)
```

我们将拟合随机森林模型并使用袋外 RMSE 估计作为内部性能指标，并使用与搜索相同的重复 10 倍交叉验证过程。 为此，我们将使用内置的 rfGA 对象。 将使用默认的 GA 运算符并执行 200 代算法。

```R
ga_ctrl <- gafsControl(functions = rfGA,
                       method = "repeatedcv",
                       repeats = 5)

## 使用与 RFE 过程相同的随机数种子，以便相同的 CV 折叠用于外部
## 重采样 
set.seed(10)
rf_ga <- gafs(x = x, y = y,
              iters = 200,
              gafsControl = ga_ctrl)
rf_ga

## 
## Genetic Algorithm Feature Selection
## 
## 100 samples
## 50 predictors
## 
## Maximum generations: 200 
## Population per generation: 50 
## Crossover probability: 0.8 
## Mutation probability: 0.1 
## Elitism: 0 
## 
## Internal performance values: RMSE, Rsquared
## Subset selection driven to minimize internal RMSE 
## 
## External performance values: RMSE, Rsquared, MAE
## Best iteration chose by minimizing external RMSE 
## External resampling method: Cross-Validated (10 fold, repeated 5 times) 
## 
## During resampling:
##   * the top 5 selected variables (out of a possible 50):
##     real1 (100%), real2 (100%), real4 (100%), real5 (100%), real3 (92%)
##   * on average, 9.3 variables were selected (min = 6, max = 15)
## 
## In the final search using the entire training set:
##    * 12 features selected at iteration 195 including:
##      real1, real2, real3, real4, real5 ... 
##    * external performance at this iteration is
## 
##        RMSE    Rsquared         MAE 
##      2.8056      0.7607      2.3640
```

10 折交叉验证重复 5 次，GA 执行 50 次。 跨重采样计算平均外部性能，这些结果用于确定最终 GA 的最佳迭代次数，以避免过度拟合。 在重采样中，每个算法结束时平均选择了 9.3 个预测变量。

 plot 函数用于监控内部袋外 RMSE 估计值的平均值以及根据 50 个样本外预测计算的外部性能估计值的平均值。 默认情况下，此函数使用 ggplot2 包。 可以将黑白主题“添加”到输出对象中：

```R
plot(rf_ga) + theme_bw()
```

![img](https://topepo.github.io/caret/ga/ga_rf_profile-1.svg)

根据这些结果，与最佳外部 RMSE 估计值相关的代为 2.81。

使用整个训练集，进行最终的 GA，在第 195 代，选择了 12 个：real1、real2、real3、real4、real5、bogus3、bogus5、bogus7、bogus8、bogus14、bogus17、bogus29。 带有这些预测器的随机森林模型是使用整个训练集创建的，这是在执行 predict.gafs 时使用的模型。

注意：对于大多数现实世界的问题，内部和外部适应度值之间的相关性有些不典型。 这是模拟性质的函数（少量不相关的信息预测变量），随机森林的 OOB 误差估计是数百棵树的产物。 你的旅费可能会改变。

#### 自定义搜索

略

#### 使用recipe

与其他特征选择例程一样，gafs 可以将数据配方作为输入。 当您的数据需要在模型之前进行预处理时，这是有利的，例如：

* 根据相互作用的规范创建虚拟变量 缺失数据插补 
* 更复杂的特征工程方法 像训练一样，配方的预处理步骤是在每个重采样中计算的。 
* 这可确保重采样统计信息捕获预处理对模型的变化和影响。

例如，使用艾姆斯住房数据。 这些数据包含许多需要转换为指标的分类预测变量以及需要处理的其他变量。 要加载（和拆分）数据：

和 train 一样，配方的预处理步骤是在每个重采样中计算的。 这可确保重采样统计信息捕获预处理对模型的变化和影响。

例如，使用艾姆斯住房数据。 这些数据包含许多需要转换为指标的分类预测变量以及需要处理的其他变量。 要加载（和拆分）数据：

```R
library(AmesHousing)
library(rsample)

# 创建数据并删除一列更多的结果。
ames <- make_ames() %>%
  select(-Overall_Qual)

ncol(ames)

## [1] 80

# 有多少因子变量？
sum(vapply(ames, is.factor, logical(1)))

## [1] 45

# 我们将使用 `rsample` 使初始分割与这些数据的其他分析保持一致。 首先设置种子以确保您获得相同的随机数
set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price", prop = 3/4)

ames_train <- training(data_split) %>% as.data.frame()
ames_test  <- testing(data_split) %>% as.data.frame()
```

这是一个对预测器集进行不同类型预处理的recipe：

```R
library(recipes)

ames_rec <- recipe(Sale_Price ~ ., data = ames_train) %>% 
  step_log(Sale_Price, base = 10) %>%
  step_other(Neighborhood, threshold = 0.05)  %>%
  step_dummy(all_nominal(), -Bldg_Type) %>%
  step_interact(~ starts_with("Central_Air"):Year_Built) %>%
  step_zv(all_predictors())%>%
  step_bs(Longitude, Latitude, options = list(df = 5))
```

如果在训练集上执行此操作，它将在原始 79 个预测器列中生成 280 个预测器列。

 让我们用 gafs 调整一些线性模型，为了计算时间，只使用 10 代算法：

```R
lm_ga_ctrl <- gafsControl(functions = caretGA, method = "cv", number = 10)

set.seed(23555)
lm_ga_search <- gafs(
  ames_rec, 
  data = ames_train,
  iters = 10, 
  gafsControl = lm_ga_ctrl,
  # now options to `train` for caretGA
  method = "lm",
  trControl = trainControl(method = "cv", allowParallel = FALSE)
) 
lm_ga_search
## 
## Genetic Algorithm Feature Selection
## 
## 2199 samples
## 273 predictors
## 
## Maximum generations: 10 
## Population per generation: 50 
## Crossover probability: 0.8 
## Mutation probability: 0.1 
## Elitism: 0 
## 
## Internal performance values: RMSE, Rsquared, MAE
## Subset selection driven to minimize internal RMSE 
## 
## External performance values: RMSE, Rsquared, MAE
## Best iteration chose by minimizing external RMSE 
## External resampling method: Cross-Validated (10 fold) 
## 
## During resampling:
##   * the top 5 selected variables (out of a possible 273):
##     Bldg_Type (100%), Bsmt_Exposure_No (100%), First_Flr_SF (100%), MS_Zoning_Residential_High_Density (100%), Neighborhood_Gilbert (100%)
##   * on average, 171.7 variables were selected (min = 150, max = 198)
## 
## In the final search using the entire training set:
##    * 155 features selected at iteration 9 including:
##      Lot_Frontage, Year_Built, Year_Remod_Add, BsmtFin_SF_2, Gr_Liv_Area ... 
##    * external performance at this iteration is
## 
##         RMSE     Rsquared          MAE 
##      0.06923      0.84659      0.04260
```

### 使用模拟退火的特征选择

#### 模拟退火

模拟退火 (SA) 是一种全局搜索方法，它对初始候选解进行小的随机变化（即扰动）。 如果扰动值的性能值优于先前的解决方案，则接受新的解决方案。 如果不是，则基于两个性能值之间的差异和搜索的当前迭代确定接受概率。 由此，可以接受次优解决方案，它可能最终在后续迭代中产生更好的解决方案。 请参阅 Kirkpatrick (1984) 或 Rutenbar (1989) 以获得更好的描述。
    

在特征选择的上下文中，解决方案是描述当前子集的二元向量。 通过随机改变子集中的少量成员来扰乱子集。

#### 内部和外部绩效评估

遗传算法页面中关于此主题的大部分讨论都与此处相关，尽管 SA 搜索不如 GA 搜索积极。 在任何情况下，这里的实现都会在重采样循环内进行 SA 搜索，并使用外部性能估计来选择合适的搜索迭代次数。

#### 基本语法

该函数的语法与之前遗传算法搜索的信息非常相似。

该函数最基本的用法是：

```R
obj <- safs(x = predictors, 
            y = outcome,
            iters = 100)
```

其中：

 x：预测变量值的数据框或矩阵 

y：结果的因子或数值向量 

iters：SA 的迭代次数 

这不是很具体。 所有的动作都在控制功能中。 这可用于指定要拟合的模型、如何进行预测和总结以及遗传操作。

假设我们想要拟合一个线性回归模型。 为此，我们可以使用 train 作为接口并通过 safs 将参数传递给该函数：

```R
ctrl <- safsControl(functions = caretSA)
obj <- safs(x = predictors, 
            y = outcome,
            iters = 100,
            safsControl = ctrl,
            ## Now pass options to `train`
            
            method = "lm")
```

也可以传入其他选项，例如 preProcess。

safsControl 的一些重要选项是：method、number、repeats、index、indexOut 等：类似于train顶部控制重采样的选项。

metric：这类似于 train 的选项，但在这种情况下，该值应该是一个命名向量，其中包含内部和外部指标的值。 如果未指定，则使用汇总函数返回的第一个值（请参阅下面的详细信息）并发出警告。 选项最大化也需要一个类似的二元素向量。 有关说明，请参阅此处的最后一个示例。

holdout：这是一个介于 [0, 1) 之间的数字，可用于保留样本以计算内部适应度值。 请注意，这与外部重采样步骤无关。 假设正在使用 10 倍的 CV。 在重采样迭代中，holdout 可用于对 90% 重采样数据的额外比例进行采样，以用于估计适应度。 这可能不是一个好主意，除非你有一个非常大的训练集并且想要避免内部重采样过程来估计适应度。

improve：一个整数（或无穷大），定义在当前子集重置为最后一次已知改进之前应该通过多少迭代而不改进适应度。

allowParallel：外部重采样循环是否应该并行运行？

有一些内置函数可以与 safs 一起使用：caretSA、rfSA 和 treebagSA。 第一个是简单的训练界面。 使用它时，如上所示，可以使用 ... 结构将参数传递给训练，并且性能的重采样估计可以用作内部适应度值。  rfSA 和 treebagSA 提供的函数避免使用 train ，它们的内部适应度估计**来自使用模型生成的袋外估计**。

#### 模拟退火示例

使用上一页中的示例，其中有 5 个真实预测变量和 40 个噪声预测变量。

我们将拟合随机森林模型并使用袋外 RMSE 估计作为内部性能指标，并使用与搜索相同的重复 10 倍交叉验证过程。 为此，我们将使用内置的 rfSA 对象。 默认 SA 运算符将用于算法的 1000 次迭代。

```R
sa_ctrl <- safsControl(functions = rfSA,
                       method = "repeatedcv",
                       repeats = 5,
                       improve = 50)

set.seed(10)
rf_sa <- safs(x = x, y = y,
              iters = 250,
              safsControl = sa_ctrl)
rf_sa
## 
## 模拟退火特征选择
## 
## 100 samples
## 50 predictors
## 
## Maximum search iterations: 250 
## Restart after 50 iterations without improvement (2.1 restarts on average)
## 
## 内部性能值：RMSE、Rsquared 
## 驱动子集选择以最小化内部 RMSE 
## 
##外部性能值：RMSE、Rsquared、MAE 
## 通过最小化外部 RMSE 选择的最佳迭代 
## 外部重采样方法：交叉验证（10 倍，重复 5 次）
## 
## 在重采样期间：
## * 前 5 个选定变量（可能的 50 个）：
##     real1 (100%), real2 (100%), real4 (100%), real5 (98%), bogus17 (88%)
##   * on average, 20.7 variables were selected (min = 12, max = 30)
## 
## 在使用整个训练集的最终搜索中：
## * 212 次迭代中选择的 21 个特征包括：
## real1、real2、real5、bogus1、bogus3 ...
## * 本次迭代的外部性能为
## 
##        RMSE    Rsquared         MAE 
##      3.3147      0.6625      2.8369
```

与 GA 一样，我们可以绘制迭代过程中的内部和外部性能。

```R
plot(rf_sa) + theme_bw()
```

这里的性能不如以前的 GA 或 RFE 解决方案。 根据这些结果，与最佳外部 RMSE 估计相关的迭代为 212，相应的 RMSE 估计为 3.31。

使用整个训练集，进行最终的 SA，在迭代 212 时，选择了 21 个：real1、real2、real5、bogus1、bogus3、bogus9、bogus10、bogus13、bogus14、bogus15、bogus19、bogus20、bogus23、bogus24、  bogus25、bogus26、bogus28、bogus31、bogus33、bogus38、bogus44。 带有这些预测器的随机森林模型是使用整个训练集创建的，这是在执行 predict.safs 时使用的模型。

#### 自定义搜索

略

#### 使用recipe

与前面关于遗传算法的部分类似，recipe可以与 safs 一起使用。 使用与之前相同的数据：

```R
library(AmesHousing)
library(rsample)

# Create the data and remove one column that is more of 
# an outcome. 
ames <- make_ames() %>%
  select(-Overall_Qual)

ncol(ames)
## [1] 80
# How many factor variables?
sum(vapply(ames, is.factor, logical(1)))
# We'll use `rsample` to make the initial split to be consistent with other
# analyses of these data. Set the seed first to make sure that you get the 
# same random numbers
set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price", prop = 3/4)

ames_train <- training(data_split) %>% as.data.frame()
ames_test  <- testing(data_split) %>% as.data.frame()

library(recipes)

ames_rec <- recipe(Sale_Price ~ ., data = ames_train) %>% 
  step_log(Sale_Price, base = 10) %>%
  step_other(Neighborhood, threshold = 0.05)  %>%
  step_dummy(all_nominal(), -Bldg_Type) %>%
  step_interact(~ starts_with("Central_Air"):Year_Built) %>%
  step_zv(all_predictors())%>%
  step_bs(Longitude, Latitude, options = list(df = 5))
## 让我们再次使用带有函数的线性模型：
lm_sa_ctrl <- safsControl(functions = caretSA,
                          method = "cv",
                          number = 10)

set.seed(23555)
lm_sa_search <- safs(
  ames_rec, 
  data = ames_train,
  iters = 10, # we probably need thousands of iterations
  safsControl = lm_sa_ctrl,
  # now options to `train` for caretSA
  method = "lm",
  trControl = trainControl(method = "cv", allowParallel = FALSE)
) 
lm_sa_search
## 
## Simulated Annealing Feature Selection
## 
## 2199 samples
## 273 predictors
## 
## Maximum search iterations: 10 
## 
## Internal performance values: RMSE, Rsquared, MAE
## Subset selection driven to minimize internal RMSE 
## 
## External performance values: RMSE, Rsquared, MAE
## Best iteration chose by minimizing external RMSE 
## External resampling method: Cross-Validated (10 fold) 
## 
## During resampling:
##   * the top 5 selected variables (out of a possible 273):
##     Roof_Style_Gambrel (70%), Latitude_bs_2 (60%), Bsmt_Full_Bath (50%), BsmtFin_Type_1_Rec (50%), Condition_1_Norm (50%)
##   * on average, 59.1 variables were selected (min = 56, max = 63)
## 
## In the final search using the entire training set:
##    * 56 features selected at iteration 4 including:
##      Year_Remod_Add, Half_Bath, TotRms_AbvGrd, Garage_Area, Enclosed_Porch ... 
##    * external performance at this iteration is
## 
##         RMSE     Rsquared          MAE 
##      0.09518      0.69840      0.07033
```

### caret的tidy表示——tidymodels包

对于R熟练的读者想必已经体验过tidyverse在数据分析工作中的强大魔力。caret的开发者在tidy的代码框架下又开发了新的tidymodels包，该包的英文介绍网站如下[网址](https://www.tidymodels.org/)

在了解了开始使用 tidymodels 需要什么之后，您可以了解更多信息并走得更远。 在此处查找文章以帮助您使用 tidymodels 框架解决特定问题。 文章分为四类： 

* [执行统计分析](https://www.tidymodels.org/learn/statistics/)

  * 具有整洁数据原则的相关性和回归基础

    同时分析多个数据集的相关测试和简单回归模型的结果。

  * 使用 tidy 数据原则进行 K-means 聚类

    总结聚类特征并估计数据集的最佳聚类数。

  * 自举重采样和整齐的回归模型

    应用bootstrap重采样估计模型参数的不确定性。

  * 使用重采样和整齐数据的假设检验

    使用灵活的函数进行统计推断的常见假设检验

  * 列联表的统计分析

    使用独立性和拟合优度测试来分析计数表。

* [创建稳健的模型 ](https://www.tidymodels.org/learn/models/)

  * 回归模型的两种方法
  * 基于神经网络的分类模型
  * 阶级失衡的二次抽样
  * 具有整齐重采样的时间序列建模
  * 使用模型系数
  * 偏最小二乘多元分析

* 调整、比较和使用您的模型 

  * 通过网格搜索进行模型

    调整通过在包含许多可能参数值的网格上进行训练，为模型选择超参数。

  * 嵌套重采样

    使用嵌套重采样估计模型的最佳超参数。

  * 分类模型的迭代贝叶斯优化

    使用迭代搜索的贝叶斯优化确定模型的最佳超参数。

  * 调整文本模型

    为预测建模准备文本数据，并使用网格和迭代搜索进行调整。

* 开发自定义建模工具

  * 创建自己的recipe步骤函数

    为数据预处理编写新的配方步骤。

  * 如何构建parsnip模型

    从现有模型实现创建parsnip模型功能。

  * 自定义性能指标

    创建新的性能指标，并将其与标尺功能集成。

  * 如何创建调整参数函数

    构建用于调整定量和定性参数的函数。

  * 为新模型对象创建自己的broom整洁方法

    Write tidy（）、glance（）和augment（）方法。

recipes列表（预处理）：

| Adaptive Synthetic Sampling Approach                         | [`step_adasyn`](https://tidymodels.github.io/themis/reference/step_adasyn.html) | themis      |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ----------- |
| 使用dplyr对行进行排序                                        | [`step_arrange`](https://tidymodels.github.io/recipes/reference/step_arrange.html) | recipes     |
| 通过袋装树木进行插补                                         | [`step_bagimpute`](https://tidymodels.github.io/recipes/reference/step_bagimpute.html) | recipes     |
| 从虚拟变量创建一个因子                                       | [`step_bin2factor`](https://tidymodels.github.io/recipes/reference/step_bin2factor.html) | recipes     |
| 非负数据的Box-Cox变换                                        | [`step_BoxCox`](https://tidymodels.github.io/recipes/reference/step_BoxCox.html) | recipes     |
| B样条基函数                                                  | [`step_bs`](https://tidymodels.github.io/recipes/reference/step_bs.html) | recipes     |
| 应用边界SMOTE算法                                            | [`step_bsmote`](https://tidymodels.github.io/themis/reference/step_bsmote.html) | themis      |
| 对中数字数据                                                 | [`step_center`](https://tidymodels.github.io/recipes/reference/step_center.html) | recipes     |
| 到类质心的距离                                               | [`step_classdist`](https://tidymodels.github.io/recipes/reference/step_classdist.html) | recipes     |
| 高相关滤波器                                                 | [`step_corr`](https://tidymodels.github.io/recipes/reference/step_corr.html) | recipes     |
| 使用正则表达式创建模式计数                                   | [`step_count`](https://tidymodels.github.io/recipes/reference/step_count.html) | recipes     |
| 日期特征生成器                                               | [`step_date`](https://tidymodels.github.io/recipes/reference/step_date.html) | recipes     |
| 数据深度                                                     | [`step_depth`](https://tidymodels.github.io/recipes/reference/step_depth.html) | recipes     |
| 离散数值变量                                                 | [`step_discretize`](https://tidymodels.github.io/recipes/reference/step_discretize.html) | recipes     |
| 基于一个因子变量对数据集进行下采样                           | [`step_downsample`](https://tidymodels.github.io/recipes/reference/step_downsample.html) | recipes     |
| Down-Sample a Data Set Based on a Factor Variable            | [`step_downsample`](https://tidymodels.github.io/themis/reference/step_downsample.html) | themis      |
| 创建虚拟变量                                                 | [`step_dummy`](https://tidymodels.github.io/recipes/reference/step_dummy.html) | recipes     |
| 将因子编码为多列                                             | [`step_embed`](https://tidymodels.github.io/embed/reference/step_embed.html) | embed       |
| 将因子转换为字符串                                           | [`step_factor2string`](https://tidymodels.github.io/recipes/reference/step_factor2string.html) | recipes     |
| Filter rows using dplyr                                      | [`step_filter`](https://tidymodels.github.io/recipes/reference/step_filter.html) | recipes     |
| Distance between two locations                               | [`step_geodist`](https://tidymodels.github.io/recipes/reference/step_geodist.html) | recipes     |
| 假日特征生成器                                               | [`step_holiday`](https://tidymodels.github.io/recipes/reference/step_holiday.html) | recipes     |
| 双曲变换                                                     | [`step_hyperbolic`](https://tidymodels.github.io/recipes/reference/step_hyperbolic.html) | recipes     |
| ICA信号提取                                                  | [`step_ica`](https://tidymodels.github.io/recipes/reference/step_ica.html) | recipes     |
| 将值转换为预定义的整数                                       | [`step_integer`](https://tidymodels.github.io/recipes/reference/step_integer.html) | recipes     |
| 创建交互变量                                                 | [`step_interact`](https://tidymodels.github.io/recipes/reference/step_interact.html) | recipes     |
| 添加截距（或常数）列                                         | [`step_intercept`](https://tidymodels.github.io/recipes/reference/step_intercept.html) | recipes     |
| 逆变换                                                       | [`step_inverse`](https://tidymodels.github.io/recipes/reference/step_inverse.html) | recipes     |
| 逆 Logit 变换                                                | [`step_invlogit`](https://tidymodels.github.io/recipes/reference/step_invlogit.html) | recipes     |
| Isomap 嵌入                                                  | [`step_isomap`](https://tidymodels.github.io/recipes/reference/step_isomap.html) | recipes     |
| 通过 K 近邻进行插补                                          | [`step_knnimpute`](https://tidymodels.github.io/recipes/reference/step_knnimpute.html) | recipes     |
| 内核 PCA 信号提取                                            | [`step_kpca`](https://tidymodels.github.io/recipes/reference/step_kpca.html) | recipes     |
| 多项式核 PCA 信号提取                                        | [`step_kpca_poly`](https://tidymodels.github.io/recipes/reference/step_kpca_poly.html) | recipes     |
| Radial Basis Function Kernel PCA Signal Extraction           | [`step_kpca_rbf`](https://tidymodels.github.io/recipes/reference/step_kpca_rbf.html) | recipes     |
| 创建滞后预测器                                               | [`step_lag`](https://tidymodels.github.io/recipes/reference/step_lag.html) | recipes     |
| 计算 lda 尺寸估计                                            | [`step_lda`](https://tidymodels.github.io/textrecipes/reference/step_lda.html) | textrecipes |
| 令牌列表变量的词形还原                                       | [`step_lemma`](https://tidymodels.github.io/textrecipes/reference/step_lemma.html) | textrecipes |
| 使用贝叶斯似然编码将监督因子转换为线性函数                   | [`step_lencode_bayes`](https://tidymodels.github.io/embed/reference/step_lencode_bayes.html) | embed       |
| Supervised Factor Conversions into Linear Functions using Likelihood Encodings | [`step_lencode_glm`](https://tidymodels.github.io/embed/reference/step_lencode_glm.html) | embed       |
| Supervised Factor Conversions into Linear Functions using Bayesian Likelihood Encodings | [`step_lencode_mixed`](https://tidymodels.github.io/embed/reference/step_lencode_mixed.html) | embed       |
| 线性组合滤波器                                               | [`step_lincomb`](https://tidymodels.github.io/recipes/reference/step_lincomb.html) | recipes     |
| 对数变换                                                     | [`step_log`](https://tidymodels.github.io/recipes/reference/step_log.html) | recipes     |
| 罗吉特变换                                                   | [`step_logit`](https://tidymodels.github.io/recipes/reference/step_logit.html) | recipes     |
| 将数值数据插补到测量阈值以下                                 | [`step_lowerimpute`](https://tidymodels.github.io/recipes/reference/step_lowerimpute.html) | recipes     |
| 使用平均值估算数值数据                                       | [`step_meanimpute`](https://tidymodels.github.io/recipes/reference/step_meanimpute.html) | recipes     |
| 使用中位数估算数值数据                                       | [`step_medianimpute`](https://tidymodels.github.io/recipes/reference/step_medianimpute.html) | recipes     |
| 使用最常见的值估算名义数据                                   | [`step_modeimpute`](https://tidymodels.github.io/recipes/reference/step_modeimpute.html) | recipes     |
| Add new variables using `mutate`                             | [`step_mutate`](https://tidymodels.github.io/recipes/reference/step_mutate.html) | recipes     |
| Mutate multiple columns                                      | [`step_mutate_at`](https://tidymodels.github.io/recipes/reference/step_mutate_at.html) | recipes     |
| 删除具有缺失值的观测值                                       | [`step_naomit`](https://tidymodels.github.io/recipes/reference/step_naomit.html) | recipes     |
| 通过删除其他类附近的点进行欠采样。                           | [`step_nearmiss`](https://tidymodels.github.io/themis/reference/step_nearmiss.html) | themis      |
| 从 tokenlist 生成 ngram                                      | [`step_ngram`](https://tidymodels.github.io/textrecipes/reference/step_ngram.html) | textrecipes |
| NNMF 信号提取                                                | [`step_nnmf`](https://tidymodels.github.io/recipes/reference/step_nnmf.html) | recipes     |
| 居中并缩放数值数据                                           | [`step_normalize`](https://tidymodels.github.io/recipes/reference/step_normalize.html) | recipes     |
| 新因子水平的简单赋值                                         | [`step_novel`](https://tidymodels.github.io/recipes/reference/step_novel.html) | recipes     |
| 自然样条基函数                                               | [`step_ns`](https://tidymodels.github.io/recipes/reference/step_ns.html) | recipes     |
| 将数字转换为因子                                             | [`step_num2factor`](https://tidymodels.github.io/recipes/reference/step_num2factor.html) | recipes     |
| 近零方差滤波器                                               | [`step_nzv`](https://tidymodels.github.io/recipes/reference/step_nzv.html) | recipes     |
| 将序数因子转换为数字分数                                     | [`step_ordinalscore`](https://tidymodels.github.io/recipes/reference/step_ordinalscore.html) | recipes     |
| 折叠一些分类级别                                             | [`step_other`](https://tidymodels.github.io/recipes/reference/step_other.html) | recipes     |
| PCA 信号提取                                                 | [`step_pca`](https://tidymodels.github.io/recipes/reference/step_pca.html) | recipes     |
| 偏最小二乘特征提取                                           | [`step_pls`](https://tidymodels.github.io/recipes/reference/step_pls.html) | recipes     |
| 正交多项式基函数                                             | [`step_poly`](https://tidymodels.github.io/recipes/reference/step_poly.html) | recipes     |
| tokenlist变量的词性过滤                                      | [`step_pos_filter`](https://tidymodels.github.io/textrecipes/reference/step_pos_filter.html) | textrecipes |
| 创建数据集的分析版本                                         | [`step_profile`](https://tidymodels.github.io/recipes/reference/step_profile.html) | recipes     |
| 将数值数据缩放到特定范围                                     | [`step_range`](https://tidymodels.github.io/recipes/reference/step_range.html) | recipes     |
| 比率变量创建                                                 | [`step_ratio`](https://tidymodels.github.io/recipes/reference/step_ratio.html) | recipes     |
| 使用正则表达式创建虚拟变量                                   | [`step_regex`](https://tidymodels.github.io/recipes/reference/step_regex.html) | recipes     |
| 将因子重新调整到所需水平                                     | [`step_relevel`](https://tidymodels.github.io/recipes/reference/step_relevel.html) | recipes     |
| 应用（平滑）校正线性变换                                     | [`step_relu`](https://tidymodels.github.io/recipes/reference/step_relu.html) | recipes     |
| Rename variables by name                                     | [`step_rename`](https://tidymodels.github.io/recipes/reference/step_rename.html) | recipes     |
| Rename multiple columns                                      | [`step_rename_at`](https://tidymodels.github.io/recipes/reference/step_rename_at.html) | recipes     |
| 通用变量过滤器                                               | [`step_rm`](https://tidymodels.github.io/recipes/reference/step_rm.html) | recipes     |
| 使用滚动窗口统计量估算数值数据                               | [`step_rollimpute`](https://tidymodels.github.io/recipes/reference/step_rollimpute.html) | recipes     |
| 应用 ROSE 算法                                               | [`step_rose`](https://tidymodels.github.io/themis/reference/step_rose.html) | themis      |
| Sample rows using dplyr                                      | [`step_sample`](https://tidymodels.github.io/recipes/reference/step_sample.html) | recipes     |
| 缩放数值数据                                                 | [`step_scale`](https://tidymodels.github.io/recipes/reference/step_scale.html) | recipes     |
| 生成基本的文本特征集                                         | [`step_sequence_onehot`](https://tidymodels.github.io/textrecipes/reference/step_sequence_onehot.html) | textrecipes |
| 随机变量                                                     | [`step_shuffle`](https://tidymodels.github.io/recipes/reference/step_shuffle.html) | recipes     |
| 使用 dplyr 按位置过滤行                                      | [`step_slice`](https://tidymodels.github.io/recipes/reference/step_slice.html) | recipes     |
| 应用 SMOTE 算法                                              | [`step_smote`](https://tidymodels.github.io/themis/reference/step_smote.html) | themis      |
| 空间符号预处理                                               | [`step_spatialsign`](https://tidymodels.github.io/recipes/reference/step_spatialsign.html) | recipes     |
| 平方根变换                                                   | [`step_sqrt`](https://tidymodels.github.io/recipes/reference/step_sqrt.html) | recipes     |
| 令牌列表变量的词干                                           | [`step_stem`](https://tidymodels.github.io/textrecipes/reference/step_stem.html) | textrecipes |
| 从 tokenlist 变量中过滤停用词                                | [`step_stopwords`](https://tidymodels.github.io/textrecipes/reference/step_stopwords.html) | textrecipes |
| 将字符串转换为因子                                           | [`step_string2factor`](https://tidymodels.github.io/recipes/reference/step_string2factor.html) | recipes     |
| 生成基本的文本特征集                                         | [`step_textfeature`](https://tidymodels.github.io/textrecipes/reference/step_textfeature.html) | textrecipes |
| 代币的词频                                                   | [`step_texthash`](https://tidymodels.github.io/textrecipes/reference/step_texthash.html) | textrecipes |
| Term frequency of tokens                                     | [`step_tf`](https://tidymodels.github.io/textrecipes/reference/step_tf.html) | textrecipes |
| 词频-令牌的逆文档频率                                        | [`step_tfidf`](https://tidymodels.github.io/textrecipes/reference/step_tfidf.html) | textrecipes |
| 时间序列特征（签名）生成器                                   | [`step_timeseries_signature`](https://business-science.github.io/timetk/reference/step_timeseries_signature.html) | timetk      |
| 根据词频过滤标记                                             | [`step_tokenfilter`](https://tidymodels.github.io/textrecipes/reference/step_tokenfilter.html) | textrecipes |
| 字符变量的标记化                                             | [`step_tokenize`](https://tidymodels.github.io/textrecipes/reference/step_tokenize.html) | textrecipes |
