---
layout:     post                    # 使用的布局（不需要改）
title:      统计建模（4）            # 标题 
subtitle:   聚类分析 #副标题
date:       2022-02-26              # 时间
author:     HZ                      # 作者
header-img: img/correlation.png    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 统计学习
    - 数学建模
---

>简述了聚类的常用方法及其R实现

### 聚类

#### [ 聚类分析基础](https://www.datanovia.com/?p=7641)：

- 用于聚类分析的数据准备和基本 R 包

- 聚类距离度量要点

  思考：选择什么类型的距离度量，欧几里得、相关性或者混合数据类型？

  是否要对数据标准化？

 

#### [分区聚类方法](https://www.datanovia.com/?p=7673)：

**分区聚类**是一种聚类方法，用于根据数据集中的观察结果的相似性将其分为多个组。算法要求分析师指定要生成的聚类数

- **K-means clustering** (MacQueen 1967)，其中，每个集群由属于该集群的数据点的中心或均值表示。K-means 方法对异常数据点和异常值很敏感。
  - 它假设数据的先验知识，并要求分析师提前选择适当数量的集群（k）
  - 获得的最终结果对聚类中心的初始随机选择很敏感。为什么会出现问题？因为，对于同一数据集上算法的每次不同运行，您可以选择不同的初始中心集。这可能会导致算法的不同运行产生不同的聚类结果。
  - 它对异常值很敏感。
  - 如果重新排列数据，很可能每次更改数据的顺序时都会得到不同的解决方案。
- **K-medoids 聚类**或**PAM**（*Partitioning Around* Medoids，Kaufman & Rousseeuw，1990），其中每个簇由簇中的一个对象表示。与 k 均值相比，PAM 对异常值不太敏感。
  - K-medoid 是 k-means 聚类的强大替代方案。这意味着，与 k-means 相比，该算法对噪声和异常值不太敏感，因为它使用 medoids 作为聚类中心而不是均值
  - 在 k-medoids 方法中，每个簇由簇内的选定对象表示。选定的对象被命名为中心点，并对应于集群中位于最中心的点。
  - PAM 算法要求用户了解数据并指明要生成的适当簇数。这可以使用函数*fviz_nbclust*进行估计。R函数*pam* ()[ *cluster* package]可用于计算PAM算法。简化格式为 pam(x, k)，其中“x”是数据，k 是要生成的簇数。
  - 之后，执行 PAM 聚类，可以使用 R 函数*fviz_cluster* () [ **factoextra**包] 来可视化结果
  - 注意，对于大数据集，*pam* ()可能需要太多的内存或太多的计算时间。在这种情况下，函数*clara* () 更可取。这对于现代计算机来说应该不是问题
- **CLARA 算法**（*Clustering Large Applications*），它是 PAM 的扩展，适用于大型数据集。
  - CLARA 不是为整个数据集寻找中心点，而是考虑具有固定大小 ( *sampsize* )的小数据样本，并应用 PAM 算法为样本生成最佳中心点集。生成的中心点的质量通过整个数据集中每个对象与其集群的中心点之间的平均差异来衡量，定义为成本函数
  - 格式：clara(x, k, metric = "euclidean", stand = FALSE,       samples = 5, pamLike = TRUE)

#### [层次聚类](https://www.datanovia.com/?p=7685)：

简介：层次聚类不需要预先指定要产生的聚类数量。层次聚类可以细分为两种类型：

1. *凝聚聚类*，其中每个观察最初都被认为是它自己的一个集群（叶子）。然后，依次合并最相似的集群，直到只有一个大集群（根）。

2. *划分聚类*，是凝聚聚类的逆向，从根开始，所有对象都包含在一个聚类中。然后依次划分最异构的簇，直到所有观察都在它们自己的簇中

凝聚聚类擅长识别小聚类。分裂聚类擅长识别大聚类。

R 包 cluster 使在 R 中执行聚类分析变得容易。它提供了函数 agnes() 和 diana() 用于计算凝聚和分裂聚类，

- 凝聚聚类

  - 算法和步骤

    聚聚类以“自下而上”的方式工作。也就是说，每个对象最初都被认为是一个单元素簇（叶子）。在算法的每一步，最相似的两个集群被组合成一个新的更大的集群（节点）。重复此过程，直到所有点都只是一个大集群（根）的成员

    注意：聚类方式选择时，ward和complete是首选

    只能根据首先融合包含这两个对象的分支的高度来得出关于两个对象的接近度的结论。我们不能使用两个对象沿水平轴的接近度作为它们相似性的标准

  - 验证簇树

    目的：评估树中的距离（即高度）是否准确反映了原始距离

    方法：计算*cophenetic*距离与*dist* () 函数生成的原始距离数据之间的相关性

  - 将树状图切割成不同的组

- 分裂聚类

- 比较树状图

  - 两个树状图的视觉比较
  - 树状图列表之间的相关矩阵

- 可视化树状图

  - 小数据集的案例
  - 具有大数据集的树状图案例：缩放、子树、PDF
  - 使用 dendexend 自定义树状图

- 热图：静态和交互式

  **热图**是另一种方式来可视化分级聚类。它也称为假彩色图像，其中数据值转换为色标。

  - R基础热图
  - 漂亮的热图
  - 交互式热图
  - 复杂的热图
  - 实际应用：基因表达数据

 

##### [聚类验证和评估策略](https://www.datanovia.com/?p=8058)：

- 评估聚类趋势

  简介：在对数据应用任何聚类方法之前，重要的是评估数据集是否包含有意义的聚类（即：非随机结构）。 如果是，那么有多少个集群。 这个过程被定义为聚类趋势的评估或聚类分析的可行性

  方法：霍普金斯统计hopkins() [clustertend package]：

  （测试数据的空间随机概率）原假设：数据集 D 是均匀分布的（即没有有意义的集群） •替代假设：数据集 D 不是均匀分布的 H值大约为.5表示均匀分布

  视觉评估(VAT 方法)：对聚类趋势进行视觉评估，VAT通过计算VAT图像中沿对角线的方形暗块的数量以视觉形式检测聚类趋势。

  

- 确定最佳簇数

  这些方法包括直接方法和统计检验方法： 

  1. 直接方法：包括优化标准，例如聚类内平方和或平均轮廓。 相应的方法分别命名为肘部方法和轮廓方法。

     肘部和平均轮廓方法的缺点是，它们仅测量全局聚类特征。 一种更复杂的方法是使用间隙统计，它提供了一种统计程序来形式化肘部/轮廓启发式，以估计最佳的聚类数。

     2. 统计检验方法：包括将证据与零假设进行比较。 一个例子是差距统计。

- 集群验证统计

  聚类验证统计可以分为 3 类：

  1. 内部聚类验证，它使用聚类过程的内部信息，在不参考外部信息的情况下评估聚类结构的优劣。 它还可以用于在没有任何外部数据的情况下**估计聚类数量和适当的聚类算法**。

     内部验证措施通常反映集群分区的紧凑性、连通性和分离性。

     两个常用指标：轮廓宽度（具有大 Si（几乎 1）的观测值非常好地聚集在一起）和邓恩指数（邓恩指数应该最大化）。 这些内部度量也可用于确定数据中的最佳聚类数。连通性的值介于 0 和无穷大之间，应将其最小化。


  2. 外部聚类验证，包括将聚类分析的结果与外部已知结果（例如外部提供的类标签）进行比较。 它测量集群标签与外部提供的类标签匹配的程度。 由于我们预先知道“真实”的聚类数，因此该方法主要用于**为特定数据集选择正确的聚类算法**。

     有两个指标来评估两个聚类的相似度，分别是修正后的Rand指标和Meila’s VI


    3. 相对聚类验证，通过改变相同算法的不同参数值来评估聚类结构（例如：改变聚类的数量 k）。 它通常用于**确定最佳聚类数**。

- 选择最佳聚类算法

  clValid package

  1. 内部度量，它使用数据中的内在信息来评估聚类的质量。 内部测量包括连通性、轮廓系数和 Dunn 指数，如上文（集群验证统计）中所述。

  2. 稳定性度量，一种特殊版本的内部度量，它通过将聚类结果与删除每一列后获得的聚类进行比较来评估聚类结果的一致性，一次一个。

     集群稳定性度量包括： • 非重叠的平均比例 (APN) • 平均距离 (AD) • 均值之间的平均距离 (ADM) • 品质因数 (FOM)

     APN、ADM 和 FOM 的取值范围为 0 到 1，较小的值对应于高度一致的聚类结果。  AD 的值介于 0 和无穷大之间，也优选较小的值。

- 计算分层聚类的 p 值

  pvclust package

  由于聚类噪声或采样错误，可以在数据集中偶然发现聚类。
    AU >= 95% 的集群被认为得到了数据的强烈支持。

 

##### [高级聚类](https://www.datanovia.com/?p=8077)：

- 分层 K 均值聚类

  K-means（上文）代表了最流行的聚类算法之一。 但是，它有一些局限性：它需要用户预先指定簇的数量并随机选择初始质心。 最终的 k-means 聚类解决方案对聚类中心的这种初始随机选择非常敏感。 每次计算 k 均值时，结果可能（略有）不同。

- 模糊聚类

  funny()

  模糊聚类被认为是软聚类，其中每个元素都有属于每个聚类的概率。 换句话说，每个元素都有一组对应于在给定集群中的程度的隶属系数。
  这与 k-means 和 k-medoid 聚类不同，其中每个对象都精确地作用于一个聚类。  K-means 和 k-medoids 聚类被称为硬聚类或非模糊聚类。
  在模糊聚类中，靠近簇中心的点可能比位于簇边缘的点在簇中的程度更高。 元素属于给定簇的程度是一个从 0 到 1 变化的数值。
   fuzzy c-means (FCM) 算法是应用最广泛的模糊聚类算法之一。 聚类的质心计算为所有点的平均值，由它们属于聚类的程度加权：

- 基于模型的聚类

  传统的聚类方法，如层次聚类（第 7 章）和 k-均值聚类（第 4 章），是启发式的，不基于形式模型。
  此外，k-means 算法通常是随机初始化的，因此不同的 k-means 运行通常会产生不同的结果。 此外，k-means 要求用户指定最佳聚类数。
  另一种方法是基于模型的聚类，它认为数据来自两个或多个聚类的混合分布与 k-means 不同，基于模型的聚类使用软分配，其中每个数据点都有属于每个聚类的概率。

  Mclust 包使用最大似然来拟合所有这些模型，具有不同的协方差矩阵参数化，适用于一系列 k 分量。
    使用贝叶斯信息准则或 BIC 选择最佳模型。 较大的 BIC 分数表明相应模型的有力证据。

- DBSCAN：基于密度的聚类

  划分方法（K-means，PAM聚类）和层次聚类适合于寻找球形或凸形聚类。换句话说，它们只适用于紧凑且分离良好的簇。此外，数据中存在的噪声和异常值也严重影响了它们。

  DBSCAN是Ester等人1996年提出的一种基于密度的聚类算法，可用于识别包含噪声和离群点的数据集中任何形状的聚类。
  基于密度的聚类方法的基本思想来源于人类直观的聚类方法
  聚类是数据空间中的密集区域，由点密度较低的区域隔开。DBSCAN算法基于“簇”和“噪声”这一直观概念。关键思想是，对于簇中的每个点，给定半径的邻域必须至少包含最少数量的点。优点：

  1.与K-means不同，DBSCAN不需要用户指定要生成的簇的数量

  2.DBSCAN可以找到任何形状的簇。集群不一定是圆形的。
  3.DBSCAN可以识别异常值
